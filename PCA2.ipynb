{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db850347-3966-4eef-b1e1-64d81dd7e691",
   "metadata": {},
   "source": [
    "# Ans : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81700366-b4c3-4a29-932c-9378a077aeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Projection in PCA:\n",
    "         Projection is the process of mapping data from a higher-dimensional space to a lower-dimensional space. \n",
    "         In PCA, this involves transforming the original data onto a new set of orthogonal axes (principal components) that maximize the\n",
    "         variance of the data.\n",
    "\n",
    "Usage in PCA: PCA projects the original data onto the directions of the principal components, which are linear combinations of the original\n",
    "variables. This projection helps in reducing the dimensionality while retaining most of the variance present in the original data.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6893de5-4aba-4113-b29a-16fdd2991f93",
   "metadata": {},
   "source": [
    "# Ans : 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08899f6c-9bf4-4758-91a9-a9553a9b83d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Optimization in PCA:\n",
    "\n",
    "Objective: The main goal of PCA is to find the principal components that capture the maximum variance in the data.\n",
    "    This is achieved by solving an optimization problem that identifies the directions (principal components) where the data varies the most.\n",
    "\n",
    "Mathematical Formulation: PCA solves an eigenvalue problem where the covariance matrix of the data is decomposed into eigenvalues and \n",
    "eigenvectors. The eigenvectors corresponding to the largest eigenvalues represent the directions of maximum variance. The optimization \n",
    "seeks to maximize the variance captured by these principal components.\n",
    "\n",
    "Outcome: By maximizing the variance, PCA reduces the dimensionality of the data while preserving as much information as possible.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb74a0-238c-4c25-9f07-b41dd723e3c3",
   "metadata": {},
   "source": [
    "# Ans : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a2248-f436-4680-b266-ca1f171e3586",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Covariance Matrices in PCA:\n",
    "\n",
    "The covariance matrix is a key component in PCA. It represents the covariance between pairs of variables in the data.\n",
    "\n",
    "Relationship: PCA involves calculating the covariance matrix of the data and then performing eigenvalue decomposition on it. The eigenvectors \n",
    "of the covariance matrix form the principal components, and the eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "Insight: The covariance matrix provides information on how variables are correlated, and PCA uses this information to identify the principal\n",
    "components that capture the most variance in the data.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae584c78-ba6c-4afd-9b66-c18e09fce7cf",
   "metadata": {},
   "source": [
    "# Ans : 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bdd474-ef0f-4023-9edd-fc4b3f9019f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Impact of Number of Principal Components:\n",
    "\n",
    "Variance Explained: The number of principal components chosen determines how much variance in the data is retained. More components mean more variance is captured, but this can also increase complexity.\n",
    "\n",
    "Trade-off: Choosing too few components may lead to loss of important information, while too many may include noise and redundant information.\n",
    "\n",
    "Performance: The optimal number of principal components balances capturing sufficient variance to accurately represent the data while avoiding\n",
    "overfitting. This choice impacts the performance of subsequent analyses or models built on the reduced data.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efec5a77-02cc-4baa-a04c-21c77d9156e0",
   "metadata": {},
   "source": [
    "# Ans : 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee38e3-6b92-42f9-ae88-7eaa23e57503",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PCA in Feature Selection:\n",
    "\n",
    "Purpose: PCA can be used to select a subset of features that explain most of the variance in the data, effectively reducing the dimensionality.\n",
    "\n",
    "Process: After computing principal components, one can select the top components that capture a significant amount of variance and use them as new features.\n",
    "\n",
    "Benefits:\n",
    "Dimensionality Reduction: Reduces the number of features, leading to simpler models and faster computations.\n",
    "Noise Reduction: Helps in removing noise and irrelevant features, improving model performance.\n",
    "Improved Interpretability: Simplifies the model, making it easier to interpret and understand the important factors.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c46c8c7-7324-4dc4-904b-d6d5e6462b40",
   "metadata": {},
   "source": [
    "# Ans : 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4740572c-8dcc-4ae2-ba94-a09ef5322b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Common Applications of PCA:\n",
    "\n",
    "Data Compression: PCA reduces data dimensionality while preserving significant variance, allowing for efficient storage and processing.\n",
    "Noise Reduction: By eliminating less significant components, PCA helps in denoising the data.\n",
    "Visualization: PCA is often used to reduce high-dimensional data to 2 or 3 dimensions for easier visualization and pattern recognition.\n",
    "Feature Extraction: In machine learning, PCA is used to extract features that capture the most important aspects of the data, which can improve the performance of predictive models.\n",
    "Image Processing: PCA is used for image compression and pattern recognition by reducing the dimensionality of image data.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8190224c-2bed-46dc-9b14-88778f071bfc",
   "metadata": {},
   "source": [
    "# Ans : 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2713ef24-84cc-455f-9b9e-db40b8b2cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Spread and Variance in PCA:\n",
    "\n",
    "Variance: In PCA, variance measures the spread of data along each principal component. High variance along a component indicates that the data is widely spread in that direction.\n",
    "\n",
    "Spread: Spread refers to how much the data points deviate from the mean along a particular direction.\n",
    "\n",
    "Relationship: PCA seeks directions where the data has the highest spread (variance). These directions (principal components) are chosen because they capture the\n",
    "    most significant patterns in the data, making them critical for reducing dimensionality while retaining information.\n",
    "    \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e5b23-09f6-4128-bc3d-519b43ec22c6",
   "metadata": {},
   "source": [
    "# Ans : 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9c56ea-273a-4332-b670-f5f0331f9856",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Identifying Principal Components with Spread and Variance:\n",
    "\n",
    "Covariance Matrix: PCA starts by calculating the covariance matrix of the data to understand how variables are correlated and to measure the spread of data.\n",
    "Eigenvalue Decomposition: PCA performs eigenvalue decomposition on the covariance matrix. The eigenvectors represent the directions (principal components) of maximum spread, and the eigenvalues indicate the amount of variance along these directions.\n",
    "Selection of Components: Principal components are selected based on the eigenvalues, with components corresponding to the largest eigenvalues chosen first, as they represent the directions with the highest variance (spread) in the data.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19cfaa8-a6d7-4993-b546-e848b13bc35e",
   "metadata": {},
   "source": [
    "# Ans : 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e198a-99de-4fbc-afd1-81b04deaa22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Handling High and Low Variance in PCA:\n",
    "\n",
    "Dimensionality Reduction: PCA reduces the dimensionality by focusing on the components with the highest variance, effectively capturing the most important patterns in the data.\n",
    "Variance Consideration: By projecting data onto the principal components with high variance, PCA emphasizes the dimensions that contribute the most to the dataâ€™s structure, while dimensions with low variance are often discarded as they contribute less to the overall variability.\n",
    "Balancing Information: PCA retains the dimensions with significant variance, ensuring that the reduced dataset captures the most critical information while discarding dimensions with low variance, which are often less informative or represent noise.\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
